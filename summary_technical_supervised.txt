** Feature Engineer , Preprocessing :

* General =
num_test  =30

X_train = dataset.data[:-num_test,:]
y_train = dataset.target[:-num_test]
X_test = dataset.data[-num_test:, :]
y_test = dataset.target[-num_test:]

#Better : X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	

* Standarize = 

	treat our data with sensitive to data with features at very different scales

	scaler = StandardScaler() no paramater as I know 

	steps :  X_scaled_train = scaler.fit_transform(X_train) ,
		 X_scaled_test = scaler.transform(X_test)
			
*Feature Encoding =
	enc =  OneHotEncoder(handle_unknown='ignore')

	steps : X_train_enc = enc.fit_transform(X_train)
		X_test_enc = enc.transform(X_test)
	
*Binarization and discretization =
# number bigger that 2.9 become 1 and reverse

	binarizer =  Binarizer(threshold=2.9) 
	fit_transform
 
*Polynomial transformation =

	PolynomialFeatures(degree=2)
	fit_transform
		
	
*Impute , Missing Data =

	imp_mean = SimpleImputer(
			missing_values=np.nan, 
			strategy='mean'/'median'
			)
	data_mean_imp = imp_mean.fit_transform(data_origin)
	or
	data_median_imp = imp_median.transform(data_origin)

*Feature selecting =

	feature_sorted = np.argsort(EstimatorAlgorithm.feature_importances_)


	

sklearn.metrics :


** Steps =

- MlAlgorithm Dirctly :

	call class , .fit(X_train,Y_train)  , .predict(X_test) final

- Logistic regression :
	call class , logreg.fit(X_train_top20, Y_train),{

	.coef_  ,
	top_20_idx = np.argsort(feature_imp)[-20:]
	X_train_top20 = X_train_enc[:, top_20_idx] 

	},.fit(X_train_top20, Y_train)

- CrossValidation :

	call class , .fit(X_train,Y_train) , .best_params_  "it look always to print it to just know ), .best_estimator_ ,.predict(X_test)

	+ accuracy = .score(X_test, Y_test) after  .best_estimator_ 
	+ .predict_proba(X_test_enc)[:, 1]




** all librairies of ml until now =

* with sklearn :

SGDClassifier: 	- loss='log_loss',
                - penalty=None ,
		- fit_intercept=True,
                - max_iter=20,
                - learning_rate='constant',
                - eta0=0.01,
		- alpha=0.0001,

RandomForestClassifier : - n_estimators=50,        	# Reduced from 100 to save memory
			  - max_depth=10,          	# Limit tree depth
    			  - min_samples_split=30,
    		 	  - n_jobs=-1,              	# Use all CPU cores
    			  - random_state= 

LogisticRegression : -max_iter=1000

xgb.XGBClassifier : - learning_rate=0.1, 		# Shrinks contribution of each tree (prevents 	overfitting)
    		    - max_depth=10,         		# Maximum depth of each tree
   		    - n_estimators=1000,    		# Number of sequential trees to train
    		    - objective='binary:logistic',  	# For binary classification
    		    - random_state=42,      		# For reproducibility
    		    - n_jobs=-1            		# Use all CPU cores




DecisionTreeRegressor : - max_depth ,
                        - min_samples_split 
			- random_state
			- min_samples_leaf

RandomForestRegressor : - n_estimators 
                        - max_depth 
			- min_samples_split 
			- random_state 

SGDRegressor :  - loss= 'squared_error',
                - learning_rate='constant'
                - random_state
		- penalty =  "l2"
		- alpha = 0.0001,
      		- eta0 = 0.2,
    		- max_iter = 100,



**SVR regression : regressor = SVR(C=100,
				 kernel='linear')

SVM  : 

clf = SVC(kernel='linear' / 'rbf'
	, C=1.0/100/300 , 
	random_state= 42,
	class_weight='balanced',
	gamma = 1/2/4
)



Z = svm.decision_function(np.c_[XX.ravel(), YY.ravel()])

*Linear Svc:
	linear_svc = LinearSVC(
    				class_weight='balanced',
    				random_state=42,
    				dual=False  # Recommended for n_samples > n_features
	)



* with tensorflow :


Linear Regression : 

	layer0 = tf.keras.layers.Dense(units=1,
                               input_shape=[X_train.shape[1]])
	model = tf.keras.Sequential([layer0])

	model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(1.0))

	model.fit(X_train, y_train, epochs=1000, verbose=True)


Logistic Regression : # return to chapter 4 to complete 
	W = tf.Variable(tf.zeros([n_features, 1]))    		# Weight matrix
	b = tf.Variable(tf.zeros([1]))                		# Bias term
	optimizer = tf.optimizers.Adam(learning_rate=0.001)  	# Adaptive learning rate

		




** CrossValidation , Tuning = 

GridSearchCV : parameter LIKE : - function that get fit like ( SGDRegressor or SGDRegressor ..)
			   	- alpha :[1e-07, 1e-06, 1e-05], # Regularization strength
    				- penalty : [None,"l2"], # Type of regularization (None or L2)
    				- eta0 : [0.03, 0.05, 0.1], # Initial learning rate for the 'constant' learning rate
    				- max_iter : [500, 1000,] # Maximum number of passes over the training data (epochs)
			   	- cv= 2 or tscv
                           	- scoring : 'neg_mean_squared_error' ,'r2'
                           	- return_train_score=True
                           	- n_jobs=-1
				- max_depth: [5, 10, 15, None],
    				- min_samples_split: [2, 10, 30, 50],
    				- class_weight : [None, 'balanced', {0:1, 1:3}],
    				- min_samples_leaf : [1, 5, 10],
    				- max_features: ['sqrt', 'log2']

KFold : - n_splits=5
	- shuffle=True
	- random_state



RandomizedSearchCV : - rf,
    		     - param_distributions=param_dist,
    		     - n_iter=30,  # Number of parameter combinations to try
    		     - cv=3,
    		     - scoring='roc_auc',
    		     - n_jobs=-1,
    		     - random_state=42,
    		     - verbose=2


TimeSeriesSplit : n_splits=3 that will be in GridSearchCV  cv=tscv as tscv = TimeSeriesSplit(n_splits=3)


**Evaluate

	* Calculate errors = 

		sklearn.metrics :- mse = mean_squared_error(y_test, predictions)
		 	 	 - rmse = np.sqrt(mse)
		 	 	 - mae = mean_absolute_error(y_test, predictions)
		 	 	 - r2 = r2_score(y_test, predictions) 

	* Calculate Score =

		sklearn.metrics : - AUC = roc_auc_score(Y_test, pred)
				  - cm = confusion_matrix(Y_test, Y_pred)  

					//sns.heatmap(cm,annot=True, fmt='d', cmap='Blues')


	* Diagnosing overfitting and underfitting =
		train_sizes, train_scores, test_scores = learning_curve(
        							estimator,
								 X, y, cv=cv, 										train_sizes=train_sizes, 								n_jobs=-1
    							)
			


	
	


 
** Load data = 

datasets.load_diabetes()  from sklearn datasets
datasets.fetch_california_housing() from data
yfinance as yf for stock market is good  yf.download("	name ,
						    	start="1990-01-01",
        						end="2023-06-30",
        						progress=False )
and transofom it to csv


** Batch training ,Online learning = 

* Online training

Sklearn :

	max_iter=1,
	.partial_fit
	
	main function : 
	for i in range(10):
    		# Get current batch
    		x_batch = X_train[i*100000:(i+1)*100000]
    		y_batch = Y_train[i*100000:(i+1)*100000]

    		# Transform features (one-hot encoding)
    		x_batch_enc = enc.transform(x_batch)

    		# Update model with current batch
    		# - classes=[0,1] specifies binary classification

    		sgd_lr_online.partial_fit(
        	x_batch_enc.toarray(),
        	y_batch,
        	classes=[0, 1])
TensorFlow :
   
  steps : 

	batch_size = 1000
	tf.data.Dataset.from_tensor_slices((X_train_tf, Y_train_tf))

	.repeat 		# Infinite epochs
	.shuffle(5000) 		# Randomize samples
	.batch(batch_size) 	# Mini-batch gradient descent
	.prefetch(1) 		# Prepare next batch while training


** Neural Network :

*sklearn  : 


nn_scikit = MLPRegressor(hidden_layer_sizes=(16, 8),                        					 activation='relu',
                         solver='adam',
                         learning_rate_init=0.001,
                         random_state=42,
                         max_iter=2000)

.fit  → .predict

*Tensorflow :

model = keras.Sequential([
    keras.layers.Dense(units=16, activation='relu'),  # Hidden Layer 1
    keras.layers.Dense(units=8, activation='relu'),   # Hidden Layer 2
    keras.layers.Dense(units=1)                       # Output Layer
])


model.compile(
	      loss='mean_squared_error',
              optimizer= tf.keras.optimizers.Adam(0.01)
	)

.fit → .predict 

*PyTorch :

model =  nn.Sequential(nn.Linear(X_train.shape[1],16),
    nn.ReLU(),
    nn.Linear(16,8),
    nn.ReLU(),
    nn.Linear(8,1)
)


loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

def train_step(model, X_train, y_train, loss_function, optimizer):
  pred_train = model(X_train)
  loss = loss_function(pred_train , y_train)

  model.zero_grad()
  loss.backward()
  optimizer.step()

  return loss.item()

for epoch in range(1000):
  loss = train_step(model,
                    X_train_torch, y_train_torch,
                    loss_function, optimizer)
  if epoch % 100 == 0:
    print(f'Epoch {epoch} Loss {loss}')

X_test_torch = torch.from_numpy(X_test.astype(np.float32))
predictions = model(X_test_torch).detach().numpy()[:, 0]





**Saving and Loading reusing Models :

* Pickle =

	#save
	pickle.dump(name, open("fileName.p", "wb" ))
	
	#load
	my_Model = pickle.load(open("fileName.p", "rb" ))


*tensorflow and Keras =
	
	
	#display 
	model.summary()

	#save 
	path = './model_tf.keras'

	model.save(path)

	#load
	new_model = tf.keras.models.load_model(path)


*Pytorch =

	#display 
	print(model)

	#save 
	path = './model.pth '
	torch.save(model, path)

	#load
	new_model = torch.load(path, weights_only=False)


	
	
	
	
	




