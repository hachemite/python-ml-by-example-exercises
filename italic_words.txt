learn from data
[Machine learning is the] field of study that gives computers the ability to
learn without being explicitly programmed.
A computer program is said to learn from experience E with respect to
some task T and some performance measure P, if its performance on T, as
measured by P, improves with experience E.
training set
training
instance
sample
model
T
E
training data
P
accuracy
Figure 1-1. The traditional approach
Figure 1-2. The machine learning approach
Figure 1-3. Automatically adapting to change
data mining
Figure 1-4. Machine learning can help humans learn
Analyzing images of products on a production line to automatically classify
them
Detecting tumors in brain scans
Automatically classifying news articles
Automatically flagging offensive comments on discussion forums
Summarizing long documents automatically
Creating a chatbot or a personal assistant
Forecasting your company’s revenue next year, based on many performance
metrics
Making your app react to voice commands
Detecting credit card fraud
Segmenting clients based on their purchases so that you can design a
different marketing strategy for each segment
k
Representing a complex, high-dimensional dataset in a clear and insightful
diagram
Recommending a product that a client may be interested in, based on past
purchases
Building an intelligent bot for a game
supervised learning
labels
Figure 1-5. A labeled training set for spam classification (an example of supervised learning)
classification
class
target
features
regression
logistic regression
Figure 1-6. A regression problem: predict a value, given an input feature (there are usually multiple
input features, and sometimes multiple output values)
target
label
target
label
features
predictors
attributes
unsupervised learning
clustering
hierarchical clustering
Figure 1-7. An unlabeled training set for unsupervised learning
Figure 1-8. Clustering
Visualization
dimensionality reduction
feature extraction
Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters
2
anomaly detection
novelty
detection
Figure 1-10. Anomaly detection
association rule learning
semi-supervised
learning
Figure 1-11. Semi-supervised learning with two classes (triangles and squares): the unlabeled
examples (circles) help classify a new instance (the cross) into the triangle class rather than the square
class, even though it is closer to the labeled squares
self-supervised learning
Figure 1-12. Self-supervised learning example: input (left) and target (right)
transfer learning
deep
neural networks
Reinforcement learning
agent
rewards
penalties
policy
Figure 1-13. Reinforcement learning
offline learning
batch learning
offline learning
model rot
data drift
online learning
mini-
batches
Figure 1-14. In online learning, a model is trained and launched into production, and then it keeps
learning as new data comes in
out-of-
core
Figure 1-15. Using online learning to handle huge datasets
learning rate
online
learning
incremental learning
generalize
measure of similarity
instance-based learning
Figure 1-16. Instance-based learning
predictions
model-
based learning
Figure 1-17. Model-based learning
Table 1-1. Does money make people happier?
Figure 1-18. Do you see a trend here?
noisy
model selection
linear model
Equation 1-1. A simple linear model
model parameters
θ
θ
Figure 1-19. A few possible linear models
θ
θ
utility function
fitness function
good
cost function
bad
training
θ
θ
type of model
fully specified model architecture
final trained model
θ
θ
Figure 1-20. The linear model that fits the training data best
Example 1-1. Training and running a linear model using Scikit-Learn
# Download and prepare the data
# Visualize the data
# Select a linear model
# Train the model
# Make a prediction for Cyprus
# Cyprus' GDP per capita in 2020
# output: [[6.30165767]]
k-nearest neighbors
k
k
inference
Figure 1-21. The importance of data versus algorithms
8
Figure 1-22. A more representative training sample
sampling noise
sampling bias
Literary Digest
Literary Digest
Literary Digest
Literary Digest
nonresponse bias
feature engineering
Feature selection
Feature extraction
all
overfitting
Figure 1-23. Overfitting the training data
w
w
regularization
θ
θ
degrees of
freedom
θ
θ
θ
θ
Figure 1-24. Regularization reduces the risk of overfitting
hyperparameter
underfitting
training set
test
set
generalization error
out-of-sample error
hold out
for that particular set
holdout validation
validation
set
development set
dev set
Figure 1-25. Model selection using holdout validation
cross-validation
train-dev set
not
Figure 1-26. When real data is scarce (right), you may use similar abundant data (left) for training and
hold out some of it in a train-dev set to evaluate overfitting; the real data is then used to evaluate data
mismatch (dev set) and to evaluate the final model’s performance (test set)
assumptions
No Free Lunch
a priori
https://homl.info/colab3
regression to the mean
Proceedings of the 26th International Conference on
Neural Information Processing Systems
θ
IEEE Intelligent Systems
Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics
Neural
Computation