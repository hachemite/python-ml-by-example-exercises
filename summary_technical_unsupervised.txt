**Download and Load  Data :


nltk.download()
nltk.download('all-corpora')

nlp = spacy.load('en_core_web_sm')

token = nlp("sentence")


from nltk.corpus import names

groups = fetch_20newsgroups()

iris = datasets.load_iris()

face_data = fetch_lfw_people(min_faces_per_person=80)
face_data = fetch_lfw_people(data_home='./',
                             min_faces_per_person=80,
                             download_if_missing=True )






**Tokenization :

word_tokenize(sentence)

[token.text for token in tokens2]


**PoS tagging:

nltk.pos_tag(tokens)


**Ner :
[(token_ent.text, token_ent.label_) for token_ent in tokens3.ents]

**Stemming :

porter_stemmer = PorterStemmer()
porter_stemmer.stem('word')

snowball_stemmer = SnowballStemmer('english')  # More aggressive than Porter


**lemmatization :

lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('word')


**Counting Vectorization :

* CountVectorizer :
count_vector = CountVectorizer(max_features ,
				lowercase ,
				stop_words,
				ngram_range,
				binary
				min_df=5,
    				max_df=0.7
 				)
data_count = count_vector.fit_transform(groups.data)
standard_features = standard_vectorizer.get_feature_names_out()



vectorizer = TfidfVectorizer(
			stop_words=initial_stop_words,
			 max_features=1000
			)
X = vectorizer.fit_transform(corpus)
** Modeling :
*NMF Class :

nmf = NMF(n_components=t, 
	random_state=42
	max_iter,
	tol
)
nmf.fit(data_cv)
nmf.components_


lda = LatentDirichletAllocation(n_components=t,
                                learning_method='batch',
				max_iter,
                                random_state=42)
lda.fit(data_cv)
lda.components_


** Text preprocessing :

"""
def preprocess_with_lemmatization(docs, lemmatizer, remove_words):
    """
    Preprocess documents using lemmatization for comparison
    """
    processed_docs = []

    for doc in docs:
        # Convert to lowercase
        doc = doc.lower()

        # Tokenize and filter
        tokens = re.findall(r'\b[a-zA-Z]{3,}\b', doc)
        filtered_tokens = [token for token in tokens if token not in remove_words]

        # Apply lemmatization
        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

        # Rejoin into a document
        processed_doc = ' '.join(lemmatized_tokens)
        processed_docs.append(processed_doc)

    return processed_docs

"""

def preprocess_with_stemming(docs, stemmer, remove_words):
    """
    Preprocess documents using stemming instead of lemmatization
    """
    processed_docs = []

    for doc in docs:
        # Convert to lowercase
        doc = doc.lower()

        # Tokenize and filter
        tokens = re.findall(r'\b[a-zA-Z]{3,}\b', doc)  # Get words with at least 3 letters
        filtered_tokens = [token for token in tokens if token not in remove_words]

        # Apply stemming
        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]

        # Rejoin into a document
        processed_doc = ' '.join(stemmed_tokens)
        processed_docs.append(processed_doc)

    return processed_docs
"""

	*stop words: 
		default_stop_words = list(_stop_words.ENGLISH_STOP_WORDS)


** K-MEANS clustering :

kmeans_sk = KMeans(
		     n_clusters , 
		     n_init='auto',
		     random_state=42
		)

kmeans_sk.fit(X)
clusters_sk = kmeans_sk.labels_
centroids_sk = kmeans_sk.cluster_centers_


** SVM clustering : 

clf = SVC(kernel='linear' / 'rbf'
	, C=1.0/100/300 , 
	random_state= 42,
	class_weight='balanced',
	gamma = 1/2/4
)

clf.fit(X_train,Y_train)

pred = clf.predict(X_test)

Z = svm.decision_function(np.c_[XX.ravel(), YY.ravel()])

*Linear Svc:
linear_svc = LinearSVC(
    class_weight='balanced',
    random_state=42,
    dual=False  # Recommended for n_samples > n_features
)


** Decomposition and Dimensionality Reduction :

pca = PCA(n_components=100,
          whiten = True ,
          random_state=42)

model = Pipeline([('pca',pca),
                  ('svc', svc)])

param_grid = {
    'pca__n_components': [100, 200, 300],
    'svc__C': [0.1, 1, 10, 100],
    'svc__gamma': [0.0001, 0.0005, 0.001, 0.005],
    'svc__kernel': ['rbf', 'linear']
}
grid_search = GridSearchCV(model, parameters_pipeline, n_jobs=-1,cv=5)

**SVR regression :

regressor = SVR(C=100, kernel='linear')
regressor.fit(X_train,y_train)
predictions = regressor.predict(X_test)



**Visualizing :

*NLP :
tsne_model = TSNE(
    n_components=2,      
    perplexity=40,       
    random_state=42,    
    learning_rate=500    
)

data_tsne = tsne_model.fit_transform(data_cleaned_count.toarray())

plt.scatter(
    data_tsne[:, 0],data_tsne[:, 1],c=groups.target,
)



* Cluster :

plt.scatter(X[:, 0], X[:, 1], c=clusters_sk)
plt.scatter(centroids_sk[:, 0], centroids_sk[:, 1], marker='*',s=200, c='r')




**Tuning :

#use elbow choosing k

for i in range(k):
   cluster_i =  np.where(clusters == i)
    sse += np.sum(np.square(X[cluster_i] - centroids[i]))
print(f'k={k} , SSE ={sse}')
sse_list[k_ind] =sse
plt.plot(k_list,sse_list)



**Score :
clf.score(X_test, Y_test)
